{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demo KFP pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install requirements:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install kfp~=1.8.14"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    Artifact,\n",
    "    Model\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Connect to client"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "client = kfp.Client(host=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If \"**host**\" if left as \"**None**\", it will use the cluster set in in kubectl current-context. E.g.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "# see current context\n",
    "kubectl config current-context\n",
    "\n",
    "# see all contexts\n",
    "kubectl config get-contexts\n",
    "\n",
    "# set context\n",
    "kubectl config use-context kind-ep\n",
    "```\n",
    "\n",
    "<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Components\n",
    "\n",
    "There are different ways to define components in KFP. Here, we use the **@component** decorator to define the components as Python function-based components.\n",
    "\n",
    "The **@component** annotation converts the function into a factory function that creates pipeline steps that execute this function. This example also specifies the base container image to run you component in."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pull data component:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\"],\n",
    "    output_component_file='pull_data_component.yaml',\n",
    ")\n",
    "def pull_data(url: str, data: Output[Dataset]):\n",
    "    \"\"\"\n",
    "    Pull data component.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(url, sep=\";\")\n",
    "    df.to_csv(data.path, index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocess component:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"],\n",
    "    output_component_file='preprocess_component.yaml',\n",
    ")\n",
    "def preprocess(\n",
    "    data: Input[Dataset],\n",
    "    scaler_out: Output[Artifact],\n",
    "    train_set: Output[Dataset],\n",
    "    test_set: Output[Dataset],\n",
    "    target: str = \"quality\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess component.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    data = pd.read_csv(data.path)\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train, test = train_test_split(data)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    train[train.drop(target, axis=1).columns] = scaler.fit_transform(train.drop(target, axis=1))\n",
    "    test[test.drop(target, axis=1).columns] = scaler.transform(test.drop(target, axis=1))\n",
    "\n",
    "    with open(scaler_out.path, 'wb') as fp:\n",
    "        pickle.dump(scaler, fp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    train.to_csv(train_set.path, index=None)\n",
    "    test.to_csv(test_set.path, index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train component:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"numpy\", \"pandas\", \"scikit-learn\", \"mlflow~=1.22.0\", \"boto3\"],\n",
    "    output_component_file='train_component.yaml',\n",
    ")\n",
    "def train(\n",
    "    train_set: Input[Dataset],\n",
    "    test_set: Input[Dataset],\n",
    "    saved_model: Output[Model],\n",
    "    alpha: float,\n",
    "    l1_ratio: float,\n",
    "    target: str = \"quality\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train component.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    import os\n",
    "    import logging\n",
    "    import pickle\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def eval_metrics(actual, pred):\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        return rmse, mae, r2\n",
    "\n",
    "    os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://mlflow-minio-service.mlflow.svc.cluster.local:9000\"\n",
    "\n",
    "    MLFLOW_EXPERIMENT_NAME = \"demo-notebook\"\n",
    "    MLFLOW_TRACKING_URI = \"http://mlflow.mlflow.svc.cluster.local:5000\"\n",
    "\n",
    "    # load data\n",
    "    train = pd.read_csv(train_set.path)\n",
    "    test = pd.read_csv(test_set.path)\n",
    "\n",
    "    # The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "    train_x = train.drop([target], axis=1)\n",
    "    test_x = test.drop([target], axis=1)\n",
    "    train_y = train[[target]]\n",
    "    test_y = test[[target]]\n",
    "\n",
    "    logger.info(f\"Using MLflow tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "    logger.info(f\"Using MLflow experiment: {MLFLOW_EXPERIMENT_NAME}\")\n",
    "    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "\n",
    "        logger.info(\"Fitting model...\")\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        logger.info(\"Predicting...\")\n",
    "        predicted_qualities = model.predict(test_x)\n",
    "\n",
    "        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "        logger.info(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
    "        logger.info(\"  RMSE: %s\" % rmse)\n",
    "        logger.info(\"  MAE: %s\" % mae)\n",
    "        logger.info(\"  R2: %s\" % r2)\n",
    "\n",
    "        logger.info(\"Logging parameters to MLflow\")\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "        # save model to mlflow\n",
    "        logger.info(\"Logging trained model\")\n",
    "        mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"ElasticnetWineModel\")\n",
    "\n",
    "        logger.info(\"Logging predictions artifact to MLflow\")\n",
    "        np.save(\"predictions.npy\", predicted_qualities)\n",
    "        mlflow.log_artifact(\n",
    "        local_path=\"predictions.npy\", artifact_path=\"predicted_qualities/\"\n",
    "        )\n",
    "\n",
    "        # save model as KFP artifact\n",
    "        logging.info(f\"Saving model to: {saved_model.path}\")\n",
    "        with open(saved_model.path, 'wb') as fp:\n",
    "            pickle.dump(model, fp, pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Pipeline\n",
    "\n",
    "Pipeline definition:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "      name='demo-pipeline',\n",
    "      description='An example pipeline that performs addition calculations.',\n",
    ")\n",
    "def pipeline(url: str, target: str, alpha: float, l1_ratio: float):\n",
    "\n",
    "    pull_task = pull_data(url=url)\n",
    "\n",
    "    preprocess_task = preprocess(data=pull_task.outputs[\"data\"])\n",
    "\n",
    "    train_task = train(\n",
    "        train_set=preprocess_task.outputs[\"train_set\"],\n",
    "        test_set=preprocess_task.outputs[\"test_set\"],\n",
    "        target=target,\n",
    "        alpha=alpha,\n",
    "        l1_ratio=l1_ratio\n",
    "    )\n",
    "    train_task.apply(use_aws_secret(secret_name=\"aws-secret\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pipeline arguments:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Specify pipeline argument values\n",
    "arguments = {\n",
    "    \"url\": \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "    \"target\": \"quality\",\n",
    "    \"alpha\": 0.5,\n",
    "    \"l1_ratio\": 0.5\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Submit run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<a href=\"/pipeline/#/experiments/details/678454c3-1cc6-4be2-9a55-9c08fdb8f702\" target=\"_blank\" >Experiment details</a>."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<a href=\"/pipeline/#/runs/details/7cc490cb-587a-4b19-b57d-95d19742d2fe\" target=\"_blank\" >Run details</a>."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "RunPipelineResult(run_id=7cc490cb-587a-4b19-b57d-95d19742d2fe)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = \"demo-run\"\n",
    "experiment_name = \"demo-experiment\"\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=pipeline,\n",
    "    run_name=run_name,\n",
    "    experiment_name=experiment_name,\n",
    "    arguments=arguments,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Check run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kubeflow Pipelines UI\n",
    "\n",
    "To access MLFlow UI, open a terminal and forward a local port to KFP UI server:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "$ kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Now KFP UI should be reachable at [http://localhost:8080/](http://localhost:8080/)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLFlow UI\n",
    "\n",
    "To access MLFlow UI, open a terminal and forward a local port to MLFlow server:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "$ kubectl -n mlflow port-forward svc/mlflow 5000:5000\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Now MLFlow's UI should be reachable at [`http://localhost:5000`](http://localhost:5000)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
